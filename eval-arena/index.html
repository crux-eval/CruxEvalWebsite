<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />   <!--It is necessary to use the UTF-8 encoding with plotly graphics to get e.g. negative signs to render correctly -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link
  rel="stylesheet"
  href="https://crux-eval.github.io/static/css/bulma.min.css"
>
<style>
  table { 
    white-space: nowrap;
    text-align: right;
  }
</style>
</head>

<body>

<section class="section">
<div class="container is-max-desktop">
<div class="columns is-centered has-text-centered">
<div class="column is-full">
    <h2 class="title is-3"> Eval-Arena: noise and errors by leaderboard showdowns </h2>
    <h3 class="title is-4"> <a href="https://github.com/crux-eval/eval-arena">Doc/Code</a> </h3>
    <div class="content has-text-justified" style="font-size: 120%;">
    <p>
      We ran pairwise match-ups on thousands of model pairs on test-based benchmarks for code generation.
      The main results and code is <a href="https://github.com/crux-eval/eval-arena">here</a>.
      <ul>
        <li><b>size</b>: number of examples in the benchmark</li>
        <li><b>p5_min</b>: the minimum difference to achieve a p-value of 0.05</li>
        <li><b>p5_max</b>: the maximum difference not achieving a p-value of 0.05</li>
        <li><b>no_solve</b>: examples not solved by any models</li>
        <li><b>tau-</b>: examples negatively correlated with the overall model quality as measured by Kendall's tau.</li>
        <li><b>details</b>: link to details for each benchmark, aggregating by the models or by examples in each benchmark.</li>
      </ul>
    </p>

    <p><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>benchmark_id</th>
      <th>size</th>
      <th>p5_min</th>
      <th>p5_max</th>
      <th>no_solve</th>
      <th>tau-</th>
      <th>link to details</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CRUXEval-input</td>
      <td>800</td>
      <td>3.2%</td>
      <td>3.9%</td>
      <td>2.4%</td>
      <td>9.4%</td>
      <td>by <a href="model_CRUXEval-input.html">models </a> | <a href="ex_CRUXEval-input.html"> examples </a></td>
    </tr>
    <tr>
      <td>CRUXEval-output</td>
      <td>800</td>
      <td>2.6%</td>
      <td>3.1%</td>
      <td>3.5%</td>
      <td>6.1%</td>
      <td>by <a href="model_CRUXEval-output.html">models </a> | <a href="ex_CRUXEval-output.html"> examples </a></td>
    </tr>
    <tr>
      <td>DS1000</td>
      <td>1000</td>
      <td>2.1%</td>
      <td>3.2%</td>
      <td>15.9%</td>
      <td>3.9%</td>
      <td>by <a href="model_DS1000.html">models </a> | <a href="ex_DS1000.html"> examples </a></td>
    </tr>
    <tr>
      <td>humaneval</td>
      <td>164</td>
      <td>6.1%</td>
      <td>9.8%</td>
      <td>3.7%</td>
      <td>1.8%</td>
      <td>by <a href="model_humaneval.html">models </a> | <a href="ex_humaneval.html"> examples </a></td>
    </tr>
    <tr>
      <td>humaneval+</td>
      <td>164</td>
      <td>6.7%</td>
      <td>9.8%</td>
      <td>4.3%</td>
      <td>1.8%</td>
      <td>by <a href="model_humaneval+.html">models </a> | <a href="ex_humaneval+.html"> examples </a></td>
    </tr>
    <tr>
      <td>lcb_codegen</td>
      <td>400</td>
      <td>2.5%</td>
      <td>4.8%</td>
      <td>24.2%</td>
      <td>1.5%</td>
      <td>by <a href="model_lcb_codegen.html">models </a> | <a href="ex_lcb_codegen.html"> examples </a></td>
    </tr>
    <tr>
      <td>mbpp</td>
      <td>378</td>
      <td>3.7%</td>
      <td>5.8%</td>
      <td>2.4%</td>
      <td>4.0%</td>
      <td>by <a href="model_mbpp.html">models </a> | <a href="ex_mbpp.html"> examples </a></td>
    </tr>
    <tr>
      <td>mbpp+</td>
      <td>378</td>
      <td>4.2%</td>
      <td>5.6%</td>
      <td>9.5%</td>
      <td>5.8%</td>
      <td>by <a href="model_mbpp+.html">models </a> | <a href="ex_mbpp+.html"> examples </a></td>
    </tr>
  </tbody>
</table></p>

    <p>The same information but in raw counts. </p>
    <p><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>benchmark_id</th>
      <th>size</th>
      <th>p5_min</th>
      <th>p5_max</th>
      <th>no_solve</th>
      <th>tau-</th>
      <th>link to details</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CRUXEval-input</td>
      <td>800</td>
      <td>26</td>
      <td>31</td>
      <td>19</td>
      <td>75</td>
      <td>by <a href="model_CRUXEval-input.html">models </a> | <a href="ex_CRUXEval-input.html"> examples </a></td>
    </tr>
    <tr>
      <td>CRUXEval-output</td>
      <td>800</td>
      <td>21</td>
      <td>25</td>
      <td>28</td>
      <td>49</td>
      <td>by <a href="model_CRUXEval-output.html">models </a> | <a href="ex_CRUXEval-output.html"> examples </a></td>
    </tr>
    <tr>
      <td>DS1000</td>
      <td>1000</td>
      <td>21</td>
      <td>32</td>
      <td>159</td>
      <td>39</td>
      <td>by <a href="model_DS1000.html">models </a> | <a href="ex_DS1000.html"> examples </a></td>
    </tr>
    <tr>
      <td>humaneval</td>
      <td>164</td>
      <td>10</td>
      <td>16</td>
      <td>6</td>
      <td>3</td>
      <td>by <a href="model_humaneval.html">models </a> | <a href="ex_humaneval.html"> examples </a></td>
    </tr>
    <tr>
      <td>humaneval+</td>
      <td>164</td>
      <td>11</td>
      <td>16</td>
      <td>7</td>
      <td>3</td>
      <td>by <a href="model_humaneval+.html">models </a> | <a href="ex_humaneval+.html"> examples </a></td>
    </tr>
    <tr>
      <td>lcb_codegen</td>
      <td>400</td>
      <td>10</td>
      <td>19</td>
      <td>97</td>
      <td>6</td>
      <td>by <a href="model_lcb_codegen.html">models </a> | <a href="ex_lcb_codegen.html"> examples </a></td>
    </tr>
    <tr>
      <td>mbpp</td>
      <td>378</td>
      <td>14</td>
      <td>22</td>
      <td>9</td>
      <td>15</td>
      <td>by <a href="model_mbpp.html">models </a> | <a href="ex_mbpp.html"> examples </a></td>
    </tr>
    <tr>
      <td>mbpp+</td>
      <td>378</td>
      <td>16</td>
      <td>21</td>
      <td>36</td>
      <td>22</td>
      <td>by <a href="model_mbpp+.html">models </a> | <a href="ex_mbpp+.html"> examples </a></td>
    </tr>
  </tbody>
</table></p>
    <ul>
      <li><a href="https://evalplus.github.io/">EvalPlus for MBPP/+, HumanEval+</a> </li>
      <li><a href="https://github.com/openai/human-eval">HumanEval</a> </li>
      <li><a href="https://github.com/google-research/google-research/tree/master/mbpp">MBPP (we used the EvalPlus version instead of original)</a> </li>
      <li><a href="https://livecodebench.github.io/leaderboard.html">LiveCodeBench</a></li>
      <li><a href="https://crux-eval.github.io/">CRUXEval</a></li>
      <li><a href="https://ds1000-code-gen.github.io/">DS1000</a></li>
    </ul>
    
</div>
</div>
</div>
</section>


</body>
</html>