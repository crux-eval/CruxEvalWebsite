<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CRUXEval: Code Reasoning, Understanding, and Execution Evaluation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">CRUXEval: Code Reasoning, Understanding, and Execution Evaluation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://minimario.github.io/">Alex Gu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=CrSf2CQAAAAJ">Baptiste Rozière</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=oIGrLpsAAAAJ">Hugh Leather</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.csail.mit.edu/asolar/">Armando Solar-Lezama</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://ai.meta.com/people/gabriel-synnaeve/">Gabriel Synnaeve</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.sidaw.xyz/">Sida I. Wang</a><sup>2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>MIT</span>
            <span class="author-block"><sup>2</sup>Meta AI Research</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/facebookresearch/cruxeval"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/cruxeval-org/cruxeval"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>HF Dataset</span>
                  </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" style="margin:0; padding:0;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified" style="font-size: 120%;">
          <p>
            The code LM community primarily relies on benchmarks like HumanEval and MBPP, 
            which test the ability to generate short code snippets from natural language 
            specifications. Many efforts overfit to these benchmarks without necessarily 
            improving other fundamentally important abilities of code models such as reasoning 
            about code execution. When it comes to reasoning about code, GPT-4 still seems to 
            have a huge edge over other models.
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section" style="margin:0; padding:0;">
  <section class="section" style="margin-bottom: 0; padding-bottom: 0;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-3">CRUXEval</h2>
          <div class="content has-text-justified" style="font-size: 120%;">
            <p>
                To better evaluate this aspect of code LMs, we design CRUXEval, a benchmark 
                of 800 Python functions and input-output pairs. The benchmark consists of two 
                tasks, CRUXEval-I (input prediction) and CRUXEval-O (output prediction) designed 
                to evaluate code reasoning, understanding, and execution. A sample of CRUXEval 
                is shown below, together with the (incorrect) outputs produced by GPT-4:
            </p>
        </div>
      </div>
  </section>

  <section class="section" style="margin-bottom: 0; padding-bottom: 0;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-3">Benchmark Construction</h2>
          <div class="content has-text-justified" style="font-size: 120%;">
            <p>
                The benchmark was constructed as follows:
                <ol>
                <li> Using Code Llama 34B, we generate a large set of candidate Python functions and inputs. </li>
                <li> We filter these candidate functions for samples which a good human programmer should be able to do without extra memory in a minute or so. </li>
                <li> 800 samples are randomly selected, a size that ensures the benchmark is both small enough to easily run and large enough to reliably compare different models. </li>
                </ol>
                We also highlight that as models improve, this generate-and-filter approach can be used to create future benchmarks that are more difficult and test other aspects of program execution.
            </p>
          </div>
        </div>
      </div>
  </section>

  <section class="section" style="margin-bottom: 0; padding-bottom: 0;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-3">Results</h2>
          <div class="content has-text-justified" style="font-size: 120%;">
            <p>
                The best model, GPT-4, achieves a pass@1 of 67% on CRUXEval-I and 63% on CRUXEval-O, while widely
                used open-source models like Code Llama 34B only achieve 47% and 44%, respectively. Despite being 
                trained on 100G of Python code and 1T of code data, it's a bit alarming that these models fail over
                half the time at simple execution prediction and code reasoning!
            </p>
          </div>
          <img src="./images/fig1.png" width="75%"/>
          <br>
          <br> 
          <div class="content has-text-justified" style="font-size: 120%;">
            <p>
                Another interesting result is that even when the variable names are replaced by placeholders like 
                x1, x2, etc., performance on CRUXEval remains relatively similar (within 3%).
            </p>
          </div>
          <img src="./images/fig2.png" width="75%"/>
        </div>
      </div>
  </section>

  <section class="section" style="margin-bottom: 0; padding-bottom: 0;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-3">Insights</h2>
          <div class="content has-text-justified" style="font-size: 120%;">
            <p>
                For base models, performance on HumanEval and CRUXEval are correlated. However, 
                finetuning breaks this correlation: distilled models (WizardCoder, Phind, Phi)
                significantly beat their base models on HumanEval but not CRUXEval.
            </p>
          </div>
          <img src="./images/fig3.png" width="75%"/>
          <br>
          <br> 
          <div class="content has-text-justified" style="font-size: 120%;">
            <p>
                Performance on CRUXEval-I and CRUXEval-O are very correlated. Because the 
                tasks seem relatively different, this suggests that the code reasoning capabilities 
                of models may generalize across tasks.
            </p>
          </div>
          <img src="./images/fig4.png" width="50%"/>
          <br>
          <br> 
          <div class="content has-text-justified" style="font-size: 120%;">
            <p>
                When comparing the predictions of different models, strong positive correlations
                are seen between sizes of the same model, between models of the same size, and 
                between instruct and base models. On average, samples that are harder for one model
                tend to be harder for other models, but worse models succeed on some examples where
                better models fail completely, showing the idiosyncrasies of each model's failures.
            </p>
          </div>
          <img src="./images/fig5.png" width="85%"/>
          <br>
          <br> 
        </div>
      </div>
  </section>

  <section class="section" style="margin-bottom: 0; padding-bottom: 0;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-3">Chain of Thought</h2>
          <div class="content has-text-justified" style="font-size: 120%;">
            <p>
                Using CoT led to some improvements, with larger boosts on output prediction 
                than input prediction. GPT-4 benefits significantly more from CoT than other 
                models, achieving the highest pass@1 of 74.8% on input prediction and 81.9% on 
                output prediction.
            </p>
          </div>
          <img src="./images/fig6.png" width="60%"/>
          <br>
          <br> 
          <div class="content has-text-justified" style="font-size: 120%;">
            <p>
                CoT increases the diversity of generated inputs and outputs, so models 
                with CoT see a larger gap between pass@1 and pass@5 score compared to 
                models without.
            </p>
          </div>
          <img src="./images/fig7.png" width="75%"/>
          <br>
          <br> 
          <div class="content has-text-justified" style="font-size: 120%;">
            <p>
                We also investigate the impact of CoT on individual samples. We classify a 
                model's performance on an example as “poor” if the model gets that example 
                wrong over half the time, and “good” otherwise. For Code Llama 13B/34B and GPT-3.5,
                we find many individual samples where CoT actually hurts the prediction accuracy. 
                This is less the case for GPT-4, where CoT improves performance for most samples.
            </p>
          </div>
          <img src="./images/fig8.png" width="75%"/>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-3">Fine-Tuning</h2>
          <div class="content has-text-justified" style="font-size: 120%;">
            <p>
                After fine-tuning Code Llama 34B on assertions very similar to those in
                our benchmark, it can match the performance of GPT-4 on both input and 
                output prediction. However, accuracy plateaus at under 70% for both tasks, 
                so simple finetuning is far from solving the benchmark.
            </p>
          </div>
          <img src="./images/fig9.png" width="75%"/>
        </div>
      </div>
  </section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The source code from this website is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">this template</a>!
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>